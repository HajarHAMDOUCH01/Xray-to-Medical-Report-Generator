import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.checkpoint
from PIL import Image
import open_clip
import math
import warnings
from typing import Optional, Tuple, Dict, Any

# Mock BioMedClip constants for demonstration.
# In a real project, these would come from your BioMedClip library.
class MockConstants:
    MODEL_NAMES = {'biomedclip': 'ViT-B-32'}
    MODEL_WEIGHTS = {'biomedclip': None} # No specific weights needed for this mock

# Use the mock constants if BioMedClip is not fully installed or for testing
try:
    from BioMedClip.constants import MODEL_NAMES, MODEL_WEIGHTS
except ImportError:
    print("BioMedClip constants not found, using mock constants for demonstration.")
    MODEL_NAMES = MockConstants.MODEL_NAMES
    MODEL_WEIGHTS = MockConstants.MODEL_WEIGHTS

# Mock transformers logging and ModelOutput for self-containment.
# In a real project, you would import these from the transformers library.
class MockLogger:
    def warn(self, message):
        print(f"WARNING: {message}")
logger = MockLogger()

class ModelOutput:
    def __init__(self, **kwargs):
        self.kwargs = kwargs
    def __getitem__(self, key):
        return self.kwargs[key]
    def __setitem__(self, key, value):
        self.kwargs[key] = value
    def __contains__(self, key):
        return key in self.kwargs
    def __repr__(self):
        return str(self.kwargs)

class BaseModelOutputWithPastAndCrossAttentions(ModelOutput):
    def __init__(self, last_hidden_state: torch.Tensor, past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, hidden_states: Optional[Tuple[torch.Tensor]] = None, attentions: Optional[Tuple[torch.Tensor]] = None, cross_attentions: Optional[Tuple[torch.Tensor]] = None):
        super().__init__(
            last_hidden_state=last_hidden_state,
            past_key_values=past_key_values,
            hidden_states=hidden_states,
            attentions=attentions,
            cross_attentions=cross_attentions,
        )

# Mock ACT2FN for activation functions
ACT2FN = {
    "gelu": F.gelu,
    "relu": F.relu,
    "tanh": torch.tanh,
    "sigmoid": torch.sigmoid,
}

# --- BERT-like Components (as provided in your initial prompt) ---

# This BertConfig is used to configure the Q-Former.
# It defines the dimensions and properties of the Q-Former's internal BERT-like layers.
# - hidden_size: The dimensionality of the query embeddings and the Q-Former's output.
# - encoder_width: The dimensionality of the input image features from BiomedCLIP (512).
# - num_query_tokens: The number of learnable query embeddings the Q-Former will use.
class BertConfig:
    def __init__(self,
                 vocab_size=30522, # Dummy, not directly used for Q-Former but needed by BertEmbeddings
                 hidden_size=768,
                 num_hidden_layers=6, # Number of Transformer layers in Q-Former
                 num_attention_heads=12,
                 intermediate_size=3072,
                 hidden_act="gelu",
                 hidden_dropout_prob=0.1,
                 attention_probs_dropout_prob=0.1,
                 max_position_embeddings=512, # Max sequence length for positional embeddings
                 type_vocab_size=2, # Dummy
                 initializer_range=0.02,
                 layer_norm_eps=1e-12,
                 pad_token_id=0,
                 add_cross_attention=True, # Crucial for Q-Former to attend to image features
                 cross_attention_freq=1, # How often cross-attention layers appear (every layer)
                 encoder_width=512, # Dimension of image features from BiomedCLIP
                 num_query_tokens=32, # Number of learnable query tokens
                 gradient_checkpointing=False,
                 position_embedding_type="absolute"):
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.num_hidden_layers = num_hidden_layers
        self.num_attention_heads = num_attention_heads
        self.intermediate_size = intermediate_size
        self.hidden_act = hidden_act
        self.hidden_dropout_prob = hidden_dropout_prob
        self.attention_probs_dropout_prob = attention_probs_dropout_prob
        self.max_position_embeddings = max_position_embeddings
        self.type_vocab_size = type_vocab_size
        self.initializer_range = initializer_range
        self.layer_norm_eps = layer_norm_eps
        self.pad_token_id = pad_token_id
        self.add_cross_attention = add_cross_attention
        self.cross_attention_freq = cross_attention_freq
        self.encoder_width = encoder_width
        self.num_query_tokens = num_query_tokens
        self.gradient_checkpointing = gradient_checkpointing
        self.position_embedding_type = position_embedding_type

class BertEmbeddings(nn.Module):
    """Construct the embeddings from word and position embeddings."""
    def __init__(self, config):
        super().__init__()
        self.word_embeddings = nn.Embedding(
            config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id
        )
        self.position_embeddings = nn.Embedding(
            config.max_position_embeddings, config.hidden_size
        )

        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

        self.register_buffer(
            "position_ids", torch.arange(config.max_position_embeddings).expand((1, -1))
        )
        self.position_embedding_type = getattr(
            config, "position_embedding_type", "absolute"
        )
        self.config = config

    def forward(
        self,
        input_ids=None,
        position_ids=None,
        query_embeds=None, # Used for Q-Former's learnable queries
        past_key_values_length=0,
    ):
        if input_ids is not None:
            seq_length = input_ids.size()[1]
        else:
            seq_length = 0

        if position_ids is None:
            position_ids = self.position_ids[
                :, past_key_values_length : seq_length + past_key_values_length
            ].clone()

        if input_ids is not None:
            embeddings = self.word_embeddings(input_ids)
            if self.position_embedding_type == "absolute":
                position_embeddings = self.position_embeddings(position_ids)
                embeddings = embeddings + position_embeddings

            if query_embeds is not None:
                embeddings = torch.cat((query_embeds, embeddings), dim=1)
        else:
            embeddings = query_embeds # For Q-Former, we directly use query_embeds as input

        embeddings = self.LayerNorm(embeddings)
        embeddings = self.dropout(embeddings)
        return embeddings


class BertSelfAttention(nn.Module):
    def __init__(self, config, is_cross_attention):
        super().__init__()
        self.config = config
        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(
            config, "embedding_size"
        ):
            raise ValueError(
                "The hidden size (%d) is not a multiple of the number of attention "
                "heads (%d)" % (config.hidden_size, config.num_attention_heads)
            )

        self.num_attention_heads = config.num_attention_heads
        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)
        self.all_head_size = self.num_attention_heads * self.attention_head_size

        self.query = nn.Linear(config.hidden_size, self.all_head_size)
        if is_cross_attention:
            # Key and Value layers for cross-attention take encoder_width (image features)
            self.key = nn.Linear(config.encoder_width, self.all_head_size)
            self.value = nn.Linear(config.encoder_width, self.all_head_size)
        else:
            # Key and Value layers for self-attention take hidden_size (query embeddings)
            self.key = nn.Linear(config.hidden_size, self.all_head_size)
            self.value = nn.Linear(config.hidden_size, self.all_head_size)

        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)
        self.position_embedding_type = getattr(
            config, "position_embedding_type", "absolute"
        )
        if (
            self.position_embedding_type == "relative_key"
            or self.position_embedding_type == "relative_key_query"
        ):
            self.max_position_embeddings = config.max_position_embeddings
            self.distance_embedding = nn.Embedding(
                2 * config.max_position_embeddings - 1, self.attention_head_size
            )
        self.save_attention = False

    def save_attn_gradients(self, attn_gradients):
        self.attn_gradients = attn_gradients

    def get_attn_gradients(self):
        return self.attn_gradients

    def save_attention_map(self, attention_map):
        self.attention_map = attention_map

    def get_attention_map(self):
        return self.attention_map

    def transpose_for_scores(self, x):
        new_x_shape = x.size()[:-1] + (
            self.num_attention_heads,
            self.attention_head_size,
        )
        x = x.view(*new_x_shape)
        return x.permute(0, 2, 1, 3)

    def forward(
        self,
        hidden_states,
        attention_mask=None,
        head_mask=None,
        encoder_hidden_states=None, # Image features for cross-attention
        encoder_attention_mask=None,
        past_key_value=None,
        output_attentions=False,
    ):
        is_cross_attention = encoder_hidden_states is not None

        if is_cross_attention:
            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))
            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))
            attention_mask = encoder_attention_mask # Use encoder's mask for cross-attention
        elif past_key_value is not None:
            key_layer = self.transpose_for_scores(self.key(hidden_states))
            value_layer = self.transpose_for_scores(self.value(hidden_states))
            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)
            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)
        else:
            key_layer = self.transpose_for_scores(self.key(hidden_states))
            value_layer = self.transpose_for_scores(self.value(hidden_states))

        mixed_query_layer = self.query(hidden_states)
        query_layer = self.transpose_for_scores(mixed_query_layer)

        past_key_value = (key_layer, value_layer)

        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))

        if (
            self.position_embedding_type == "relative_key"
            or self.position_embedding_type == "relative_key_query"
        ):
            seq_length = hidden_states.size()[1]
            position_ids_l = torch.arange(
                seq_length, dtype=torch.long, device=hidden_states.device
            ).view(-1, 1)
            position_ids_r = torch.arange(
                seq_length, dtype=torch.long, device=hidden_states.device
            ).view(1, -1)
            distance = position_ids_l - position_ids_r
            positional_embedding = self.distance_embedding(
                distance + self.max_position_embeddings - 1
            )
            positional_embedding = positional_embedding.to(
                dtype=query_layer.dtype
            )

            if self.position_embedding_type == "relative_key":
                relative_position_scores = torch.einsum(
                    "bhld,lrd->bhlr", query_layer, positional_embedding
                )
                attention_scores = attention_scores + relative_position_scores
            elif self.position_embedding_type == "relative_key_query":
                relative_position_scores_query = torch.einsum(
                    "bhld,lrd->bhlr", query_layer, positional_embedding
                )
                relative_position_scores_key = torch.einsum(
                    "bhrd,lrd->bhlr", key_layer, positional_embedding
                )
                attention_scores = (
                    attention_scores
                    + relative_position_scores_query
                    + relative_position_scores_key
                )

        attention_scores = attention_scores / math.sqrt(self.attention_head_size)
        if attention_mask is not None:
            attention_scores = attention_scores + attention_mask

        attention_probs = nn.Softmax(dim=-1)(attention_scores)

        if is_cross_attention and self.save_attention:
            self.save_attention_map(attention_probs)
            attention_probs.register_hook(self.save_attn_gradients)

        attention_probs_dropped = self.dropout(attention_probs)

        if head_mask is not None:
            attention_probs_dropped = attention_probs_dropped * head_mask

        context_layer = torch.matmul(attention_probs_dropped, value_layer)

        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
        context_layer = context_layer.view(*new_context_layer_shape)

        outputs = (
            (context_layer, attention_probs) if output_attentions else (context_layer,)
        )

        outputs = outputs + (past_key_value,)
        return outputs


class BertSelfOutput(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, hidden_states, input_tensor):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.LayerNorm(hidden_states + input_tensor)
        return hidden_states


class BertAttention(nn.Module):
    def __init__(self, config, is_cross_attention=False):
        super().__init__()
        self.self = BertSelfAttention(config, is_cross_attention)
        self.output = BertSelfOutput(config)
        self.pruned_heads = set()

    def prune_heads(self, heads):
        if len(heads) == 0:
            return
        # Simplified prune_heads for demonstration, actual implementation would involve more
        warnings.warn("`prune_heads` is not fully implemented for this example.")

    def forward(
        self,
        hidden_states,
        attention_mask=None,
        head_mask=None,
        encoder_hidden_states=None,
        encoder_attention_mask=None,
        past_key_value=None,
        output_attentions=False,
    ):
        self_outputs = self.self(
            hidden_states,
            attention_mask,
            head_mask,
            encoder_hidden_states,
            encoder_attention_mask,
            past_key_value,
            output_attentions,
        )
        attention_output = self.output(self_outputs[0], hidden_states)

        outputs = (attention_output,) + self_outputs[
            1:
        ]
        return outputs


class BertIntermediate(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
        if isinstance(config.hidden_act, str):
            self.intermediate_act_fn = ACT2FN[config.hidden_act]
        else:
            self.intermediate_act_fn = config.hidden_act

    def forward(self, hidden_states):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.intermediate_act_fn(hidden_states)
        return hidden_states


class BertOutput(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, hidden_states, input_tensor):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.LayerNorm(hidden_states + input_tensor)
        return hidden_states


class BertLayer(nn.Module):
    def __init__(self, config, layer_num):
        super().__init__()
        self.config = config
        self.chunk_size_feed_forward = config.chunk_size_feed_forward
        self.seq_len_dim = 1
        self.attention = BertAttention(config)
        self.layer_num = layer_num
        if (
            self.config.add_cross_attention
            and layer_num % self.config.cross_attention_freq == 0
        ):
            self.crossattention = BertAttention(
                config, is_cross_attention=self.config.add_cross_attention
            )
            self.has_cross_attention = True
        else:
            self.has_cross_attention = False
        self.intermediate = BertIntermediate(config)
        self.output = BertOutput(config)

        # Q-Former often uses separate FFNs for queries
        self.intermediate_query = BertIntermediate(config)
        self.output_query = BertOutput(config)

    def forward(
        self,
        hidden_states, # These are the query embeddings
        attention_mask=None,
        head_mask=None,
        encoder_hidden_states=None, # These are the image features
        encoder_attention_mask=None,
        past_key_value=None,
        output_attentions=False,
        query_length=0, # Length of query sequence
    ):
        # Self-attention on the query embeddings
        self_attn_past_key_value = (
            past_key_value[:2] if past_key_value is not None else None
        )
        self_attention_outputs = self.attention(
            hidden_states,
            attention_mask,
            head_mask,
            output_attentions=output_attentions,
            past_key_value=self_attn_past_key_value,
        )
        attention_output = self_attention_outputs[0]
        outputs = self_attention_outputs[1:-1] # Contains attentions if output_attentions is True

        present_key_value = self_attention_outputs[-1]

        # Cross-attention to image features, applied only to query tokens
        if query_length > 0:
            # Only the query part of the hidden_states attends to encoder_hidden_states
            query_attention_output = attention_output[:, :query_length, :]

            if self.has_cross_attention:
                assert (
                    encoder_hidden_states is not None
                ), "encoder_hidden_states must be given for cross-attention layers"
                cross_attention_outputs = self.crossattention(
                    query_attention_output,
                    attention_mask, # This mask applies to queries in cross-attention
                    head_mask,
                    encoder_hidden_states,
                    encoder_attention_mask, # Mask for image features
                    output_attentions=output_attentions,
                )
                query_attention_output = cross_attention_outputs[0]
                outputs = (
                    outputs + cross_attention_outputs[1:-1]
                )

            # Apply feed-forward network to the query part
            layer_output = apply_chunking_to_forward(
                self.feed_forward_chunk_query,
                self.chunk_size_feed_forward,
                self.seq_len_dim,
                query_attention_output,
            )
            # If there were other tokens (e.g., text tokens concatenated with queries),
            # process them with the regular FFN. In Q-Former, usually only queries.
            if attention_output.shape[1] > query_length:
                layer_output_text = apply_chunking_to_forward(
                    self.feed_forward_chunk,
                    self.chunk_size_feed_forward,
                    self.seq_len_dim,
                    attention_output[:, query_length:, :],
                )
                layer_output = torch.cat([layer_output, layer_output_text], dim=1)
        else:
            # If no specific query_length, process all hidden_states with regular FFN
            layer_output = apply_chunking_to_forward(
                self.feed_forward_chunk,
                self.chunk_size_feed_forward,
                self.seq_len_dim,
                attention_output,
            )
        outputs = (layer_output,) + outputs
        outputs = outputs + (present_key_value,)
        return outputs

    def feed_forward_chunk(self, attention_output):
        intermediate_output = self.intermediate(attention_output)
        layer_output = self.output(intermediate_output, attention_output)
        return layer_output

    def feed_forward_chunk_query(self, attention_output):
        intermediate_output = self.intermediate_query(attention_output)
        layer_output = self.output_query(intermediate_output, attention_output)
        return layer_output


class BertEncoder(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.layer = nn.ModuleList(
            [BertLayer(config, i) for i in range(config.num_hidden_layers)]
        )

    def forward(
        self,
        hidden_states, # Query embeddings
        attention_mask=None,
        head_mask=None,
        encoder_hidden_states=None, # Image features
        encoder_attention_mask=None,
        past_key_values=None,
        use_cache=None,
        output_attentions=False,
        output_hidden_states=False,
        return_dict=True,
        query_length=0, # Length of query sequence
    ):
        all_hidden_states = () if output_hidden_states else None
        all_self_attentions = () if output_attentions else None
        all_cross_attentions = (
            () if output_attentions and self.config.add_cross_attention else None
        )

        next_decoder_cache = () if use_cache else None

        for i in range(self.config.num_hidden_layers):
            layer_module = self.layer[i]
            if output_hidden_states:
                all_hidden_states = all_hidden_states + (hidden_states,)

            layer_head_mask = head_mask[i] if head_mask is not None else None
            past_key_value = past_key_values[i] if past_key_values is not None else None

            if getattr(self.config, "gradient_checkpointing", False) and self.training:
                if use_cache:
                    logger.warn(
                        "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
                    )
                    use_cache = False

                def create_custom_forward(module):
                    def custom_forward(*inputs):
                        return module(
                            *inputs, past_key_value, output_attentions, query_length
                        )
                    return custom_forward

                layer_outputs = torch.utils.checkpoint.checkpoint(
                    create_custom_forward(layer_module),
                    hidden_states,
                    attention_mask,
                    layer_head_mask,
                    encoder_hidden_states,
_attention_mask,
                )
            else:
                layer_outputs = layer_module(
                    hidden_states,
                    attention_mask,
                    layer_head_mask,
                    encoder_hidden_states,
                    encoder_attention_mask,
                    past_key_value,
                    output_attentions,
                    query_length,
                )

            hidden_states = layer_outputs[0]
            if use_cache:
                next_decoder_cache += (layer_outputs[-1],)
            if output_attentions:
                all_self_attentions = all_self_attentions + (layer_outputs[1],)
                if self.config.add_cross_attention and layer_module.has_cross_attention:
                    # Cross-attention output is the third element if it exists
                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)


        if output_hidden_states:
            all_hidden_states = all_hidden_states + (hidden_states,)

        if not return_dict:
            return tuple(
                v
                for v in [
                    hidden_states,
                    next_decoder_cache,
                    all_hidden_states,
                    all_self_attentions,
                    all_cross_attentions,
                ]
                if v is not None
            )
        return BaseModelOutputWithPastAndCrossAttentions(
            last_hidden_state=hidden_states,
            past_key_values=next_decoder_cache,
            hidden_states=all_hidden_states,
            attentions=all_self_attentions,
            cross_attentions=all_cross_attentions,
        )


class BertPooler(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.activation = nn.Tanh()

    def forward(self, hidden_states):
        first_token_tensor = hidden_states[:, 0]
        pooled_output = self.dense(first_token_tensor)
        pooled_output = self.activation(pooled_output)
        return pooled_output

# Mock for `apply_chunking_to_forward`
def apply_chunking_to_forward(
    forward_fn, chunk_size, seq_len_dim, *input_tensors
):
    """
    This function takes a forward function, a chunk size, and a dimension to chunk
    along. It then applies the forward function to each chunk of the input tensors.
    """
    if chunk_size > 0:
        return torch.cat(
            [
                forward_fn(*[t.narrow(seq_len_dim, i, chunk_size) for t in input_tensors])
                for i in range(0, input_tensors[0].shape[seq_len_dim], chunk_size)
            ],
            dim=seq_len_dim,
        )
    return forward_fn(*input_tensors)


# --- Q-Former Implementation ---

class Qformer(nn.Module):
    def __init__(self, config: BertConfig):
        super().__init__()
        self.config = config

        # Learnable query tokens, these are the 'queries' that will interact with image features
        self.query_tokens = nn.Parameter(torch.zeros(1, config.num_query_tokens, config.hidden_size))
        self.query_tokens.data.normal_(mean=0.0, std=config.initializer_range) # Initialize queries

        # The BERT-like encoder for the Q-Former.
        # It handles self-attention among queries and cross-attention to image features.
        self.bert_encoder = BertEncoder(config)

        # A projection layer if the output dimension of the Q-Former needs to match
        # a specific input dimension for BioGPT (e.g., if BioGPT has a different hidden size).
        # Assuming BioGPT expects the same hidden_size as Q-Former's output for simplicity.
        # If BioGPT's input embedding size is different, you'd add a linear layer here:
        # self.output_projection = nn.Linear(config.hidden_size, biogpt_hidden_size)

    def forward(
        self,
        image_features: torch.Tensor, # Output from BiomedCLIPEncoder (batch_size, feature_dim)
        image_attention_mask: Optional[torch.Tensor] = None, # Mask for image features if needed
        # In this setup, image_features are usually the sequence of visual tokens,
        # so we'll treat them as `encoder_hidden_states` in the BERT encoder.
        # If BiomedCLIP returns a single feature vector, you'll need to expand its dimension.
        # For open_clip, `encode_image` usually returns (batch_size, feature_dim).
        # We'll reshape it to (batch_size, 1, feature_dim) or (batch_size, num_visual_tokens, feature_dim)
        # depending on how you want the cross-attention to work.
        # Let's assume `image_features` is already (batch_size, sequence_length, feature_dim)
        # or we will expand it.
    ):
        batch_size = image_features.shape[0]
        device = image_features.device

        # Prepare query embeddings: repeat across the batch
        query_tokens = self.query_tokens.expand(batch_size, -1, -1) # (batch_size, num_query_tokens, hidden_size)

        # Prepare attention mask for queries (all 1s as queries are not masked)
        # This mask applies to the self-attention within the Q-Former.
        query_attention_mask = torch.ones(query_tokens.shape[:-1], dtype=torch.long, device=device)

        # If image_features is a single vector per image, unsqueeze it to be a sequence of 1 token
        if image_features.dim() == 2: # (batch_size, feature_dim)
            encoder_hidden_states = image_features.unsqueeze(1) # (batch_size, 1, feature_dim)
        else: # Assumed (batch_size, num_visual_tokens, feature_dim)
            encoder_hidden_states = image_features

        # Create an attention mask for the image features if not provided.
        # Assuming all visual tokens are attended to.
        if image_attention_mask is None:
            image_attention_mask = torch.ones(
                encoder_hidden_states.shape[:-1], dtype=torch.long, device=device
            ) # (batch_size, num_visual_tokens)

        # Get extended attention mask for cross-attention
        # BERT-like models expect an attention mask that's compatible with their internal logic.
        # This typically involves expanding the mask to be broadcastable to attention heads.
        extended_image_attention_mask = self.bert_encoder.get_extended_attention_mask(
            image_attention_mask, encoder_hidden_states.shape[:-1], device, is_decoder=False
        )


        # Pass through the BERT encoder.
        # The query_tokens are the `hidden_states` for the BERT encoder.
        # The image_features are the `encoder_hidden_states` for cross-attention.
        encoder_outputs = self.bert_encoder(
            hidden_states=query_tokens,
            attention_mask=query_attention_mask, # Self-attention mask for queries
            encoder_hidden_states=encoder_hidden_states, # Image features from BiomedCLIP
            encoder_attention_mask=extended_image_attention_mask, # Cross-attention mask for image features
            return_dict=True,
            query_length=query_tokens.shape[1] # Crucial to tell BertLayer which part are queries
        )

        # The output `last_hidden_state` will contain the enriched query embeddings
        # after attending to the image features.
        output_query_embeddings = encoder_outputs.last_hidden_state

        # Optional: Project the output if BioGPT expects a different dimension
        # if hasattr(self, 'output_projection'):
        #    output_query_embeddings = self.output_projection(output_query_embeddings)

        return output_query_embeddings # (batch_size, num_query_tokens, hidden_size)


# --- Full Integrated Model ---

class XrayReportGenerator(nn.Module):
    def __init__(self, biomedclip_model_name, biomedclip_weights_path, biogpt_config, qformer_config):
        super().__init__()
        self.biomedclip_encoder = BiomedCLIPEncoder(
            model_name=biomedclip_model_name,
            weights_path=biomedclip_weights_path
        )

        # Ensure qformer_config.encoder_width matches biomedclip_encoder.feature_dim
        assert qformer_config.encoder_width == self.biomedclip_encoder.feature_dim, \
            "Q-Former encoder_width must match BiomedCLIP feature_dim"

        self.qformer = Qformer(qformer_config)

        # Initialize BioGPT decoder (mock or actual from transformers)
        # For simplicity, we'll use a mock here.
        # In a real scenario, you'd load BioGPT like:
        # from transformers import BioGptForCausalLM, BioGptTokenizer
        # self.tokenizer = BioGptTokenizer.from_pretrained("microsoft/biogpt")
        # self.biogpt_decoder = BioGptForCausalLM.from_pretrained("microsoft/biogpt")
        # You might need to adjust BioGPT's input embeddings to accept the Q-Former output.
        # A common approach is to add a linear layer from qformer_config.hidden_size
        # to biogpt_hidden_size, and then concatenate these with BioGPT's own token embeddings.
        
        # For this example, we'll mock BioGPT as a simple linear layer
        # that takes the Q-Former output and "generates" some text embeddings.
        # In reality, BioGPT would be an autoregressive language model.
        
        # This is a placeholder. BioGPT needs to be able to ingest the Q-Former output.
        # One way is to treat the Q-Former output as 'prefix' tokens.
        # Another is to use it as cross-attention input within BioGPT's decoder layers,
        # if BioGPT's architecture supports it (like many encoder-decoder models).
        # Given BioGPT is a *decoder-only* LLM, you'd typically concatenate the
        # Q-Former output with your initial text prompts.
        
        # Let's assume BioGPT's hidden size is the same as Q-Former's output for simplicity.
        # If not, you'd need a projection layer here.
        # Example: `self.biogpt_projection = nn.Linear(qformer_config.hidden_size, biogpt_config.hidden_size)`
        
        # Mock BioGPT for demonstration:
        # It needs to accept input embeddings, not just token IDs.
        # We'll simulate its `generate` method.
        class MockBioGPT(nn.Module):
            def __init__(self, hidden_size, vocab_size):
                super().__init__()
                self.vocab_size = vocab_size
                self.embedding_size = hidden_size
                # A simple linear layer to simulate generation
                self.output_layer = nn.Linear(hidden_size, vocab_size)

            def forward(self, input_embeddings, attention_mask=None, **kwargs):
                # In a real BioGPT, this would involve more complex attention and feed-forward layers
                # For demonstration, just pass through a linear layer
                logits = self.output_layer(input_embeddings)
                return {'logits': logits} # Simulate output structure

            def generate(self, input_embeddings, max_new_tokens=50, **kwargs):
                # Simple mock generation: just takes the first embedding and repeats a token
                # In real BioGPT, this is a complex autoregressive process
                generated_tokens = []
                current_embedding = input_embeddings[:, 0, :] # Use the first query embedding
                
                for _ in range(max_new_tokens):
                    # Simulate passing embedding through a decoder step
                    # In a real LLM, this would be `decoder(input_ids, encoder_hidden_states=...)`
                    # For a decoder-only LLM, it's `decoder(input_ids, past_key_values=...)`
                    # Here, we just take the current state and predict the next token
                    
                    # Mock: Just predict a random token or a specific token.
                    # This is highly simplified!
                    logits = self.output_layer(current_embedding) # (batch_size, vocab_size)
                    next_token_id = torch.argmax(logits, dim=-1) # (batch_size,)
                    generated_tokens.append(next_token_id)
                    
                    # For real generation, you'd embed `next_token_id` and use it as `current_embedding`
                    # This mock doesn't fully simulate that as it needs `vocab_size` and embedding layer.
                    # For this example, we'll just return the query embeddings directly.
                    break # Stop after one "step" for simplicity of mock.
                
                # For actual BioGPT, you would concatenate Q-Former output with actual token embeddings
                # and pass them through `biogpt_decoder.generate`
                # The output would be token IDs.
                
                # For this mock, we just return the Q-Former output as the "context" for BioGPT
                return input_embeddings # Returning query embeddings as "context" for BioGPT

        # Ensure BioGPT's hidden size matches Q-Former's output for direct concatenation/use
        # You will need to properly define biogpt_config to reflect BioGPT's actual architecture.
        # For this mock, we'll use Q-Former's hidden size.
        biogpt_mock_hidden_size = qformer_config.hidden_size
        biogpt_mock_vocab_size = 50257 # Common vocab size for GPT-like models
        self.biogpt_decoder = MockBioGPT(biogpt_mock_hidden_size, biogpt_mock_vocab_size)
        
        # A projection if BioGPT's embedding layer is different size
        # self.qformer_output_to_biogpt_input_projection = nn.Linear(qformer_config.hidden_size, biogpt_config.hidden_size)


    def forward(self, image_path, prompt_text=None, max_new_tokens=50):
        # 1. Encode Image with BiomedCLIP
        image_features = self.biomedclip_encoder.encode_image(image_path)
        # `image_features` is (1, 512) from `encode_image` method.
        # Q-Former expects (batch_size, sequence_length, feature_dim).
        # We need to unsqueeze it to (1, 1, 512) to represent one visual token.
        image_features_expanded = image_features.unsqueeze(1) # (batch_size, 1, 512)

        # 2. Process Image Features with Q-Former
        # The Q-Former output will be (batch_size, num_query_tokens, hidden_size)
        query_embeddings = self.qformer(image_features_expanded)

        # 3. Integrate Q-Former output with BioGPT for report generation
        # This is the crucial part that depends on how BioGPT is used.

        # For a decoder-only LLM like BioGPT, you typically concatenate the Q-Former
        # output embeddings with the input text embeddings (e.g., your prompt).
        
        # If `prompt_text` is provided, you would tokenize it and get its embeddings.
        # For simplicity, let's assume no explicit prompt_text for now,
        # and BioGPT starts generating directly from the Q-Former's query embeddings.
        
        # To make BioGPT "see" the image information, we feed the `query_embeddings`
        # as the initial input embeddings to BioGPT's generate method.
        # NOTE: A real BioGPT would typically take `input_ids` (token IDs) and
        # then look up its own embeddings. To pass `query_embeddings`, you might need
        # to directly manipulate BioGPT's internal forward pass or use a model that
        # explicitly supports initial input embeddings (like `transformers` models often do).
        
        # A common pattern for decoder-only LLMs like BioGPT in multimodal settings:
        # 1. Image features -> Q-Former -> Query Embeddings (Q_emb)
        # 2. Text prompt -> Text Embeddings (T_emb)
        # 3. Concatenate: `initial_input_embeddings = torch.cat([Q_emb, T_emb], dim=1)`
        # 4. Pass `initial_input_embeddings` to BioGPT's `generate` function's `inputs_embeds` argument.
        
        # For this example, we'll just pass the Q-Former output directly.
        # This is a simplification, as BioGPT's `generate` expects token IDs,
        # or `inputs_embeds` if you're providing custom embeddings.

        # Mock BioGPT generation using query_embeddings as initial context
        # The `generate` method here is heavily mocked for simplicity.
        # In a real setup, you would use:
        # from transformers import BioGptTokenizer
        # tokenizer = BioGptTokenizer.from_pretrained("microsoft/biogpt")
        # generated_ids = self.biogpt_decoder.generate(
        #     inputs_embeds=query_embeddings,
        #     max_new_tokens=max_new_tokens,
        #     # Add other generation parameters like do_sample, top_k, top_p, num_beams etc.
        # )
        # generated_report = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
        
        generated_report_mock = self.biogpt_decoder.generate(
            input_embeddings=query_embeddings, 
            max_new_tokens=max_new_tokens
        )
        
        # For this mock, `generated_report_mock` is just the query_embeddings.
        # We convert it to a simple string representation for the output.
        return f"Generated Report (Mock): Information derived from {image_path} via Q-Former.\nQuery embeddings shape: {generated_report_mock.shape}"


# --- Example Usage ---
if __name__ == "__main__":
    # 1. Define Q-Former Configuration
    # These parameters are critical for how the Q-Former behaves.
    # hidden_size: Should ideally match the hidden size of BioGPT or a common intermediate size.
    # encoder_width: Must match the output dimension of your BiomedCLIP encoder (512).
    # num_query_tokens: How many discrete "chunks" of visual information the Q-Former distills.
    #                   A higher number means more detailed information can be extracted,
    #                   but also increases computational cost for BioGPT.
    qformer_config = BertConfig(
        hidden_size=768, # Common hidden size for LLMs
        encoder_width=512, # BiomedCLIP feature dimension
        num_hidden_layers=6, # Fewer layers than full BERT, but enough for distillation
        num_attention_heads=12,
        intermediate_size=3072,
        add_cross_attention=True,
        cross_attention_freq=1, # Every layer will cross-attend to image features
        num_query_tokens=32 # A good starting point for queries
    )

    # 2. Initialize the full model
    # BioGPT config can be a dummy for now, as we're mocking it.
    biogpt_dummy_config = {} # Replace with actual BioGPT config if using real BioGPT
    
    # Assuming you have biomedclip model weights and names defined or accessible
    biomedclip_model_name = MODEL_NAMES['biomedclip']
    biomedclip_weights_path = MODEL_WEIGHTS['biomedclip']

    model = XrayReportGenerator(
        biomedclip_model_name=biomedclip_model_name,
        biomedclip_weights_path=biomedclip_weights_path,
        biogpt_config=biogpt_dummy_config, # Pass your BioGPT config here
        qformer_config=qformer_config
    )

    # 3. Create a dummy image file for testing
    dummy_image_path = "dummy_xray.png"
    Image.new('RGB', (224, 224), color = 'red').save(dummy_image_path)

    # 4. Generate a report
    print(f"Generating report for: {dummy_image_path}")
    generated_report = model(dummy_image_path)
    print(generated_report)

    # Clean up dummy image
    import os
    os.remove(dummy_image_path)
