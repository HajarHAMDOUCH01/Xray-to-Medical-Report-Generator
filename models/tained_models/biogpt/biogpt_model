from transformers import BioGptForCausalLM, BioGptTokenizer

class XrayReportGenerator(nn.Module):
    def __init__(self, biomedclip_model_name, biomedclip_weights_path, qformer_config):
        super().__init__()
        self.biomedclip_encoder = BiomedCLIPEncoder(
            model_name=biomedclip_model_name,
            weights_path=biomedclip_weights_path
        )

        assert qformer_config.encoder_width == self.biomedclip_encoder.feature_dim, \
            "Q-Former encoder_width must match BiomedCLIP feature_dim"

        self.qformer = Qformer(qformer_config)

        # --- ACTUAL BIOGPT LOADING ---
        self.tokenizer = BioGptTokenizer.from_pretrained("microsoft/biogpt")
        self.biogpt_decoder = BioGptForCausalLM.from_pretrained("microsoft/biogpt")

        # Get BioGPT's hidden size for potential projection
        biogpt_hidden_size = self.biogpt_decoder.config.hidden_size

        # If Q-Former output hidden_size doesn't match BioGPT's input embedding size,
        # you need a projection layer. This is common.
        # Example: if Q-Former hidden_size is 768 but BioGPT's is 1024
        if qformer_config.hidden_size != biogpt_hidden_size:
            self.qformer_output_to_biogpt_input_projection = nn.Linear(
                qformer_config.hidden_size, biogpt_hidden_size
            )
        else:
            self.qformer_output_to_biogpt_input_projection = None

        # You might need to add special tokens to BioGPT's tokenizer and model
        # For visual tokens, typically a placeholder token like <image_tokens>
        # Check BioGPT tokenizer for existing special tokens and add new ones if needed.
        # Example:
        # num_added_tokens = self.tokenizer.add_special_tokens({'additional_special_tokens': ['<img_token>']})
        # self.biogpt_decoder.resize_token_embeddings(len(self.tokenizer))
        # self.image_placeholder_token_id = self.tokenizer.convert_tokens_to_ids('<img_token>')
        # And then repeat this token embedding `num_query_tokens` times.
        # A simpler approach is to directly pass `inputs_embeds` as discussed.

        # Store the ID for the end-of-sentence token for generation stopping criteria
        self.eos_token_id = self.tokenizer.eos_token_id

        def forward(self, image_path, prompt_text: Optional[str] = None, max_new_tokens=50, num_beams=1, do_sample=False, top_k=None, top_p=None):
            # 1. Encode Image with BiomedCLIP
            image_features = self.biomedclip_encoder.encode_image(image_path)
            image_features_expanded = image_features.unsqueeze(1) # (batch_size, 1, 512)

            # 2. Process Image Features with Q-Former
            query_embeddings = self.qformer(image_features_expanded) # (batch_size, num_query_tokens, qformer_hidden_size)

            # 3. Project Q-Former output if necessary
            if self.qformer_output_to_biogpt_input_projection:
                query_embeddings = self.qformer_output_to_biogpt_input_projection(query_embeddings)
                # Now: (batch_size, num_query_tokens, biogpt_hidden_size)

            # 4. Prepare Textual Prompt Embeddings (if any)
            # This is where you prepare your actual text input (e.g., "Report:")
            # or just an empty prompt to let the model generate freely based on the image.
            if prompt_text:
                prompt_token_ids = self.tokenizer(prompt_text, return_tensors="pt", add_special_tokens=False).input_ids
                prompt_token_ids = prompt_token_ids.to(query_embeddings.device)
                # Get embeddings for the prompt text from BioGPT's embedding layer
                text_embeddings = self.biogpt_decoder.get_input_embeddings()(prompt_token_ids)
                # Now: (batch_size, prompt_length, biogpt_hidden_size)

                # Concatenate query embeddings and text embeddings
                # The LLM will now process visual information first, then the text prompt.
                input_embeddings = torch.cat([query_embeddings, text_embeddings], dim=1)
                # Create an attention mask for the combined input
                # Query tokens are not masked, text tokens are not masked.
                # (batch_size, num_query_tokens + prompt_length)
                input_attention_mask = torch.ones(input_embeddings.shape[:-1], dtype=torch.long, device=input_embeddings.device)
            else:
                # If no text prompt, start generation directly from query embeddings
                input_embeddings = query_embeddings
                input_attention_mask = torch.ones(input_embeddings.shape[:-1], dtype=torch.long, device=input_embeddings.device)


            # 5. Generate Report with BioGPT
            # The key is to pass inputs_embeds to the generate method.
            generated_output = self.biogpt_decoder.generate(
                inputs_embeds=input_embeddings,
                attention_mask=input_attention_mask,
                max_new_tokens=max_new_tokens,
                num_beams=num_beams, # For beam search
                do_sample=do_sample, # For sampling-based generation
                top_k=top_k,
                top_p=top_p,
                eos_token_id=self.eos_token_id, # Stop generation when EOS token is generated
                pad_token_id=self.tokenizer.pad_token_id, # Required for batch inference
            )

            # Decode the generated token IDs back to text
            # `generated_output` will be a tensor of token IDs: (batch_size, sequence_length)
            # Assuming batch_size is 1 for now for simplicity in decoding
            generated_report = self.tokenizer.decode(generated_output[0], skip_special_tokens=True)

            return generated_report